{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\envs\\python3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "\n",
    "from neo.core import SpikeTrain\n",
    "from quantities import ms, s, Hz\n",
    "from elephant.statistics import mean_firing_rate\n",
    "from elephant.statistics import time_histogram, instantaneous_rate\n",
    "from elephant.kernels import GaussianKernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speriatus 189139306\n",
      "Trials keept = 313\n",
      "Trials excluded = 315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\envs\\python3\\lib\\site-packages\\ipykernel_launcher.py:72: RuntimeWarning: invalid value encountered in less\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:495: UserWarning: Binning discarded 1 last spike(s) in the input spiketrain.\n",
      "  n=n_spikes - n_spikes_binned))\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:495: UserWarning: Binning discarded 2 last spike(s) in the input spiketrain.\n",
      "  n=n_spikes - n_spikes_binned))\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:990: UserWarning: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "  'behaviour.'.format(num_rounding_corrections))\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:274: UserWarning: Correcting a rounding error in the calculation of n_bins by increasing n_bins by 1. You can set tolerance=None to disable this behaviour.\n",
      "  warnings.warn('Correcting a rounding error in the calculation '\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:495: UserWarning: Binning discarded 3 last spike(s) in the input spiketrain.\n",
      "  n=n_spikes - n_spikes_binned))\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:990: UserWarning: Correcting 2 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "  'behaviour.'.format(num_rounding_corrections))\n",
      "C:\\Users\\David\\AppData\\Roaming\\Python\\Python35\\site-packages\\elephant\\conversion.py:495: UserWarning: Binning discarded 4 last spike(s) in the input spiketrain.\n",
      "  n=n_spikes - n_spikes_binned))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for monkey_directory in ['Speriatus', 'Mojo']:\n",
    "    path = 'C:\\\\Users\\\\David\\\\Desktop\\\\IDIBAPS\\\\Gottlib_data\\\\data files\\\\distractor paper data only\\\\' +monkey_directory\n",
    "    ### path = 'C:\\\\Users\\\\David\\\\Desktop\\\\fast_process_gottlieb\\\\' +monkey_directory\n",
    "    Monkey = path.split('\\\\')[-1]\n",
    "    for file_to_use in os.listdir(path):  #\n",
    "        neuron = file_to_use.split('.')[0]\n",
    "        print(Monkey, neuron)\n",
    "        #########\n",
    "        ### Open file\n",
    "        f = scipy.io.loadmat(os.path.join(path, file_to_use))\n",
    "        ########\n",
    "        ### Check if there is spiking data inside\n",
    "        if np.shape(f['data']['spikes'][0][0][0]) ==(0,): \n",
    "            print('No data')\n",
    "            \n",
    "        else:\n",
    "            ########\n",
    "            ### Spikes \n",
    "            df_spike_time=pd.DataFrame(f['data']['spikes'][0][0][0][0][:, :])\n",
    "            df_spike_time.columns = [str(i) for i in range(0, np.shape(df_spike_time)[1])]\n",
    "            ########\n",
    "            ### Events\n",
    "            Dict_events={}\n",
    "            for i in range(0, len(f['data']['events'][0][0][0])     ):\n",
    "                events = pd.DataFrame( f['data']['events'][0][0][0][i])\n",
    "                events.columns=['time', 'code']\n",
    "                Dict_events[str(i)]= events\n",
    "            #\n",
    "            ########\n",
    "            ### Descriptors & Bad trials\n",
    "            ## Use the Descriptors to make the booleans to select the HIT trials\n",
    "            lists=[]\n",
    "            for T in range(0, len(f['data']['descriptors'][0][0][0])):\n",
    "                lists.append( [f['data']['descriptors'][0][0][0][T][i][0] for i in range(len(f['data']['descriptors'][0][0][0][T]) )] )\n",
    "            #\n",
    "            Descriptors = pd.DataFrame(lists)\n",
    "            Descriptors=Descriptors.transpose()\n",
    "            Descriptors.columns = [str(i) for i in range(0, np.shape(Descriptors)[1])]\n",
    "            # 6 \"failed\" // performance code, as defined below \n",
    "            boolean_hit_trials = Descriptors.iloc[6].isin([1]) #Descriptors.iloc[6,:]==1\n",
    "            ########\n",
    "            ### Bad trials\n",
    "            ## take off the bad trials\n",
    "            bad = pd.DataFrame(np.array([f['data']['bad'][0][0][0][x][0][0] for x in range(0, len(f['data']['bad'][0][0][0]))]))\n",
    "            bad = bad.transpose()\n",
    "            bad.columns = [str(i) for i in range(0, np.shape(df_spike_time)[1])]\n",
    "            #each column in a trial; if 1, discard it\n",
    "            boolean_bad_trials = bad.iloc[0,:]!=1\n",
    "            ########\n",
    "            ########\n",
    "            ### Boolean combinging success trials and correct\n",
    "            boolean_keep = boolean_hit_trials & boolean_bad_trials \n",
    "            print('Trials keept = ' +str(sum(boolean_keep)))\n",
    "            print('Trials excluded = ' + str(np.shape(df_spike_time)[1] - sum(boolean_keep) ))\n",
    "            ########\n",
    "            ### df_spikes_correct\n",
    "            indexes_trials_keep = np.array([str(i) for i in range(0, np.shape(df_spike_time)[1])])[boolean_keep]\n",
    "            df_spikes = df_spike_time[list(indexes_trials_keep)]\n",
    "            df_spikes = df_spikes.iloc[4:, :] #remove the first 4 rows, no spikes\n",
    "            df_spikes = df_spikes.transpose()[df_spikes.iloc[0,:]<9999].transpose() #remove columns full of nans\n",
    "            ########\n",
    "            ########\n",
    "            ########\n",
    "            ### Get the firing rate of each trial (Gaussian fit and auto)\n",
    "            ### turorial https://elephant.readthedocs.io/en/latest/tutorials/statistics.html\n",
    "            trials_=[]            \n",
    "            Descriptors_use = Descriptors[list(df_spikes.columns)]           \n",
    "            for TRIAL in list(df_spikes.columns):\n",
    "                ###### firing rates\n",
    "                times_spikes_all = df_spikes[TRIAL].values # get trial spike times\n",
    "                times_spikes = times_spikes_all[times_spikes_all<9999] ##remove nans in the trial (at the end)\n",
    "                times_spikes = times_spikes[times_spikes>0] #no negative timings for spikes\n",
    "                stop_time =  times_spikes.max()\n",
    "                spiketrain1 = SpikeTrain(times_spikes, units=ms, t_stop=stop_time) #spiketrain\n",
    "                histogram_rate = time_histogram([spiketrain1], 10*ms, output='rate') #sampling rate of 10 Suzuki & Gottlieb, increase for less temporal resolution. \n",
    "                #inst_rate = instantaneous_rate(spiketrain1, sampling_period=10*ms) \n",
    "                gaus_rate = instantaneous_rate(spiketrain1, sampling_period=10*ms, kernel=GaussianKernel(15*ms)) #s.d of Suzuki & Gottlieb \n",
    "                times_ = gaus_rate.times.rescale(ms)\n",
    "                #firing_auto = inst_rate.rescale(histogram_rate.dimensionality).magnitude.flatten()\n",
    "                firing_gauss = gaus_rate.rescale(histogram_rate.dimensionality).magnitude.flatten()\n",
    "                df_trial = pd.DataFrame({'times':times_, 'firing_gauss':firing_gauss}) #, 'firing_auto': firing_auto})\n",
    "                                \n",
    "                ####### descriptive data\n",
    "                df_trial['trial']=TRIAL \n",
    "                df_trial['TDOA'] = Descriptors_use[TRIAL].iloc[20]\n",
    "                df_trial['TDconf'] = Descriptors_use[TRIAL].iloc[24]\n",
    "                df_trial['targetloc'] = Descriptors_use[TRIAL].iloc[1]\n",
    "                df_trial['distLoc'] = Descriptors_use[TRIAL].iloc[19]\n",
    "                df_trial['fixationtime'] = Descriptors_use[TRIAL].iloc[9]\n",
    "                df_trial['failed'] = Descriptors_use[TRIAL].iloc[6]\n",
    "                df_trial['saccadeChoice'] = Descriptors_use[TRIAL].iloc[18]\n",
    "                df_trial['cueDuration'] = Descriptors_use[TRIAL].iloc[10]\n",
    "                df_trial['distDur'] = Descriptors_use[TRIAL].iloc[21]\n",
    "                df_trial['monkey'] = Monkey\n",
    "                df_trial['neuron'] = neuron           \n",
    "                trials_.append(df_trial)\n",
    "            #\n",
    "            df_neuron=pd.concat(trials_)\n",
    "            ########\n",
    "            ########\n",
    "            ### Normalization\n",
    "            # Get the mean of each time in the control condition (T in RF no dist), then get the max of this means\n",
    "            max_fr_gauss_normalize = df_neuron.loc[(df_neuron['TDconf']==0), ['firing_gauss', 'times']].groupby('times').describe()['firing_gauss']['mean'].max()\n",
    "            #max_fr_auto_normalize = df_neuron.loc[(df_neuron['TDconf']==0), ['firing_auto', 'times']].groupby('times').describe()['firing_auto']['mean'].max()\n",
    "            df_neuron['norm_firing_gauss'] = df_neuron['firing_gauss']/max_fr_gauss_normalize\n",
    "            #df_neuron['norm_firing_auto'] = df_neuron['firing_auto']/max_fr_auto_normalize\n",
    "            ########\n",
    "            ########\n",
    "            ######## Save the dataframe as an excel with the name of the neuron\n",
    "            path_save = 'C:\\\\Users\\\\David\\\\Desktop\\\\IDIBAPS\\\\Gottlib_data\\\\firing_rates'\n",
    "            path_save_neuron = os.path.join(path_save, neuron+'.xlsx')\n",
    "            df_neuron.to_excel(path_save_neuron)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
